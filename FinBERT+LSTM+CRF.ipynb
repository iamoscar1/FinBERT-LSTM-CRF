{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0484ea4",
   "metadata": {},
   "source": [
    "# Load the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7deff463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import gc\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, random_split\n",
    "from torchcrf import CRF# We need this encapsulated for complicated CRF components\n",
    "import torchcrf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0416da",
   "metadata": {},
   "source": [
    "# Read in the datasets from .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tags = []\n",
    "with open('ner_datasets/source_BIO_2014_cropus.txt','r') as f1:\n",
    "    for line in f1:\n",
    "        sentences.append(line)\n",
    "with open('ner_datasets/target_BIO_2014_cropus.txt','r') as f2:\n",
    "    for line in f2:\n",
    "        tags.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51256c",
   "metadata": {},
   "source": [
    "# Set the number of data samples we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d682b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:1000]\n",
    "tags = tags[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15581a0",
   "metadata": {},
   "source": [
    "# Split the sentences/labels into token by token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c87cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].split()\n",
    "for i in range(len(tags)):\n",
    "    tags[i] = tags[i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c978d33",
   "metadata": {},
   "source": [
    "# Count the exsiting lable types in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562d8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B_T', 'O', 'I_PER', 'B_ORG', 'I_LOC', 'B_LOC', 'I_T', 'I_ORG', 'B_PER'}\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for i in tags:\n",
    "    for tag in i:\n",
    "        labels.add(tag)\n",
    "print (labels)\n",
    "print (len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716656d",
   "metadata": {},
   "source": [
    "# Transfer the label symbol into label indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a0f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2index = {\n",
    "    \"B_PER\":0,\n",
    "    \"I_PER\":1,\n",
    "    \"B_LOC\":2,\n",
    "    \"I_LOC\":3,\n",
    "    \"B_T\":4,\n",
    "    \"I_T\":5,\n",
    "    \"B_ORG\":6,\n",
    "    \"I_ORG\":7,\n",
    "    \"O\":8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a094f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in tags:\n",
    "    for i in range(len(tag)):\n",
    "        tag[i] = label2index[tag[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb157ec",
   "metadata": {},
   "source": [
    "# Tokenize and index the input sentences with pretrained Tokenizer (HuggingFcae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6bb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "Tokenizer = AutoTokenizer.from_pretrained(\"FinBERT_L-12_H-768_A-12_pytorch\",add_special_tokens=False)\n",
    "# No special tokens needed for NER task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9c1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [Tokenizer.convert_tokens_to_ids(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6734f3",
   "metadata": {},
   "source": [
    "# Count the lengths of of all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4754832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 2\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "min_length = 100\n",
    "for i in tags:\n",
    "    max_length = max(max_length,len(i))\n",
    "    min_length = min(min_length,len(i))    \n",
    "print (max_length,min_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552adda",
   "metadata": {},
   "source": [
    "# Set Train and Test datastes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d3cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[:900]\n",
    "test_sentences = sentences[900:]\n",
    "\n",
    "train_labels = tags[:900]\n",
    "test_labels = tags[900:]\n",
    "\n",
    "train_masking = []\n",
    "test_masking = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0124f2c",
   "metadata": {},
   "source": [
    "# Generate the \"input_ids\", \"labels\", and \"masking\" tensors of the Train and Test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025b26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    if len(train_sentences[i])>=100:\n",
    "        train_sentences[i] = train_sentences[i][:100]\n",
    "        train_labels[i] = train_labels[i][:100]\n",
    "        train_masking.append([1 for i in range(100)])\n",
    "    else:\n",
    "        train_sentences[i] = train_sentences[i]+[0 for i in range(100-len(train_sentences[i]))]\n",
    "        train_labels[i] = train_labels[i]+[0 for i in range(100-len(train_labels[i]))]\n",
    "        train_masking.append([1 for i in range(len(train_sentences[i]))]+[0 for i in range(100-len(train_sentences[i]))])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    if len(test_sentences[i])>=100:\n",
    "        test_sentences[i] = test_sentences[i][:100]\n",
    "        test_labels[i] = test_labels[i][:100]\n",
    "        test_masking.append([1 for i in range(100)])\n",
    "    else:\n",
    "        test_sentences[i] = test_sentences[i]+[0 for i in range(100-len(test_sentences[i]))]\n",
    "        test_labels[i] = test_labels[i]+[0 for i in range(100-len(test_labels[i]))]\n",
    "        test_masking.append([1 for i in range(len(test_sentences[i]))]+[0 for i in range(100-len(test_sentences[i]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ec526",
   "metadata": {},
   "source": [
    "# Constrcut the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b07c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, maskings):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.maskings = maskings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\":torch.tensor(self.encodings[idx])}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['maskings'] = torch.tensor(self.maskings[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NERDataset(train_sentences,train_labels,train_masking)\n",
    "test_dataset = NERDataset(test_sentences,test_labels,test_masking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab510a63",
   "metadata": {},
   "source": [
    "# Set Batch-Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54d5e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb5afb",
   "metadata": {},
   "source": [
    "# Load the datasets into Dataloader, batchlize them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdea8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f52624",
   "metadata": {},
   "source": [
    "# Check the content in each batch from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d91dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in test_dataloader:\n",
    "#    print (batch)\n",
    "#    break\n",
    "#batch[\"maskings\"].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7fe32e",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5233996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FinBERT_L-12_H-768_A-12_pytorch were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BERTBiLSTMCRF(nn.Module):\n",
    "    def __init__(self,num_tags = None):\n",
    "        super(BERTBiLSTMCRF, self).__init__()\n",
    "        # Get the sequence encoding from FinBERT\n",
    "        self.base_model = AutoModel.from_pretrained('FinBERT_L-12_H-768_A-12_pytorch')\n",
    "        # The hyper-parameters for LSTM\n",
    "        self.word_embeds = 768# output dimensions from FinBERT\n",
    "        self.hidden_dim = 1024# double-layer: 2* actual hidden_size\n",
    "        self.num_tags = num_tags# the number of unique labels\n",
    "        # build the lstm\n",
    "        self.lstm = nn.LSTM(self.word_embeds, self.hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first = True)\n",
    "        # map from the dimension of lstm outputs to the dimension of num_tags\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.num_tags)\n",
    "        # build the CRF\n",
    "        self.crf = torchcrf.CRF(self.num_tags,batch_first=True)\n",
    "    \n",
    "    def forward(self, sequence = None, labels = None, maskings = None):\n",
    "        #Get the batch_size\n",
    "        batch_size = sequence.size()[0]\n",
    "        # add maskings, because various input lengths and the corresponding paddings\n",
    "        outputs = self.base_model(sequence,attention_mask = maskings)#output tuple:(LastLayerSequenceOuput,PoolerOutput)\n",
    "        \n",
    "        \n",
    "        ##########################################################\n",
    "        #######The process to help LSTM get rid of padding########\n",
    "        \n",
    "        lengths = []# the list to store the real length of each input\n",
    "        for i in range(batch_size):\n",
    "            lengths.append(maskings[i,:].tolist().count(1))\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(outputs[0], torch.Tensor(lengths).long(), batch_first=True)\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        hidden_state = torch.randn(2, batch_size, self.hidden_dim//2)\n",
    "        cell_state = torch.randn(2, batch_size, self.hidden_dim//2)\n",
    "        # now run through LSTM\n",
    "        X  = self.lstm(X,(hidden_state,cell_state))[0]#The output of lstm is (hidden_output, cell_output)\n",
    "        # undo the packing operation\n",
    "        X = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)[0]\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1,sequence.size()[1] ,X.shape[2])\n",
    "        \n",
    "        #lstm_outputs = self.lstm(outputs[0])[0]\n",
    "        \n",
    "        ##########################################################\n",
    "        ##########################################################\n",
    "        \n",
    "        emission_scores = self.hidden2tag(X)# map from 1024 to 9\n",
    "        # decode and get the predicted labels\n",
    "        predictions = self.crf.decode(emission_scores, mask = maskings.bool())# bool()!!!!!\n",
    "        # calculate losses\n",
    "        if labels is not None:\n",
    "            loss = -self.crf.forward(emission_scores, labels, mask = maskings.bool(), reduction='sum')\n",
    "            return (loss,predictions)\n",
    "        else:\n",
    "            return predictions\n",
    "model = BERTBiLSTMCRF(num_tags = 9)\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "##############################################################################################\n",
    "#############The example of the use of CRF modules.\n",
    "#############The order of seq_length and batch_size can be different, they are all supported. Or\n",
    "#############set the batch_first=True (defalut is batch_first=False)\n",
    "#tags = torch.tensor([[0, 1], [2, 4], [3, 1]], dtype=torch.long)  # (seq_length, batch_size)\n",
    "#emissions = torch.randn(seq_length, batch_size, num_tags)\n",
    "#model(emissions, tags)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd073a",
   "metadata": {},
   "source": [
    "# Set up epoch number and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "605df660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "epochs = 3\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#total_steps = len(train_data_loader) * epochs\n",
    "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019387",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b63733f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [16:22<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is:  33213.80809020996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [16:25<00:00,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is:  8971.87890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [16:08<00:00,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is:  4793.221740722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    for step, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        # Extract contents from each batch. They are of the size B*Seq_len\n",
    "        sent = batch[\"input_ids\"]\n",
    "        lab = batch[\"labels\"]\n",
    "        mask = batch[\"maskings\"]\n",
    "        outputs = model(sent,lab,mask)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss+=loss.item()\n",
    "        optimizer.step()\n",
    "    print (\"loss is: \", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b499956",
   "metadata": {},
   "source": [
    "# Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54375b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:28<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(test_dataloader),total=len(test_dataloader)):\n",
    "        sent = batch[\"input_ids\"]\n",
    "        mask = batch[\"maskings\"]\n",
    "        #There is no \"lables\" in the test step\n",
    "        outputs = model(sent,maskings = mask)\n",
    "        results.append(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2060f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_resulsts = []\n",
    "for i in results:\n",
    "    final_resulsts+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1e13d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_resulsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef590ece",
   "metadata": {},
   "source": [
    "# Minor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5469b848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 0),\n",
       " ([8], 1),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8),\n",
       " ([8], 8)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = test_sentences[0]\n",
    "sent = torch.Tensor(sentence).long().view(1,-1)\n",
    "result = model(sent)\n",
    "list(zip(result,test_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "300bf456",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "61f48bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n"
     ]
    }
   ],
   "source": [
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7bf1fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8b242d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8a4e425c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 3, 2, 8, 6, 7, 1, 9, 5]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ff26eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.Tensor([[1,1,1,1,0,0],[1,1,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb5404fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1,:].tolist().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306ee794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchrua\n",
      "  Downloading torchrua-0.3.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/aooscar/anaconda/envs/py3/lib/python3.8/site-packages (from torchrua) (1.21.2)\n",
      "Collecting einops\n",
      "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: einops, torchrua\n",
      "Successfully installed einops-0.3.2 torchrua-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchrua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
